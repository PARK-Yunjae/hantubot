# hantubot_prod/hantubot/reporting/study.py
"""
ìœ ëª©ë¯¼ ê³µë¶€ë²• (100ì¼ ê³µë¶€) - SQLite + ë‰´ìŠ¤ ìˆ˜ì§‘ + LLM ìš”ì•½ í†µí•© ë²„ì „
"""
import os
import time
import json
from datetime import datetime
from typing import List, Dict, Optional

from pykrx import stock
import google.generativeai as genai

from .logger import get_logger
from .study_db import get_study_db, StudyDatabase
from ..utils.stock_filters import is_eligible_stock
from ..providers import NaverNewsProvider

logger = get_logger(__name__)


def run_daily_study(broker, notifier, force_run=False):
    """
    ìœ ëª©ë¯¼ ê³µë¶€ë²• ë©”ì¸ í•¨ìˆ˜ - SQLite ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„
    
    Args:
        broker: ë¸Œë¡œì»¤ ì¸ìŠ¤í„´ìŠ¤ (ë¯¸ì‚¬ìš©, ì‹œê·¸ë‹ˆì²˜ í˜¸í™˜ì„± ìœ ì§€)
        notifier: ì•Œë¦¼ ì¸ìŠ¤í„´ìŠ¤
        force_run: Trueë©´ ì¤‘ë³µ ì²´í¬ ë¬´ì‹œí•˜ê³  ê°•ì œ ì‹¤í–‰
    """
    logger.info("=" * 80)
    logger.info("ìœ ëª©ë¯¼ ê³µë¶€ë²• (100ì¼ ê³µë¶€) ì‹œì‘ - SQLite + ë‰´ìŠ¤ ìˆ˜ì§‘ ë²„ì „")
    logger.info("=" * 80)
    
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    study_mode = os.getenv('STUDY_MODE', 'sqlite')  # sqlite / gsheet / both
    
    # ë‚ ì§œ ì„¤ì •
    today_str = datetime.now().strftime("%Y%m%d")
    today_date = datetime.now().strftime("%Y-%m-%d")
    
    # DB ì´ˆê¸°í™”
    try:
        db = get_study_db()
    except Exception as e:
        logger.error(f"Failed to initialize database: {e}", exc_info=True)
        notifier.send_alert(f"âŒ ìœ ëª©ë¯¼ ê³µë¶€ë²• DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", level='error')
        return
    
    # 1. ì¤‘ë³µ ì‹¤í–‰ ì²´í¬
    if not force_run:
        existing_run = db.get_run(today_str)
        if existing_run and existing_run['status'] in ['success', 'partial']:
            logger.info(f"Today's study for {today_str} already completed. Skipping.")
            return
    
    # 2. Run ì‹œì‘
    try:
        run_id = db.start_run(today_str)
        logger.info(f"Started new study run: {today_str} (run_id={run_id})")
    except Exception as e:
        logger.error(f"Failed to start run: {e}", exc_info=True)
        notifier.send_alert(f"âŒ ìœ ëª©ë¯¼ ê³µë¶€ë²• ì‹œì‘ ì‹¤íŒ¨: {e}", level='error')
        return
    
    stats = {
        'candidates': 0,
        'news_collected': 0,
        'summaries_generated': 0,
        'errors': []
    }
    
    try:
        # ========== ë‹¨ê³„ 1: ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ==========
        logger.info("[1/4] ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        candidates = collect_market_data(today_str, db)
        stats['candidates'] = len(candidates)
        
        if not candidates:
            logger.info("No candidates found for today. Ending run.")
            db.end_run(today_str, 'success', stats=stats)
            return
        
        logger.info(f"âœ… {len(candidates)}ê°œ í›„ë³´ ì¢…ëª© ë°œê²¬ ë° DB ì €ì¥ ì™„ë£Œ")
        
        # ========== ë‹¨ê³„ 2: ë‰´ìŠ¤ ìˆ˜ì§‘ ==========
        logger.info("[2/4] ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        news_stats = collect_news_for_candidates(today_str, candidates, db)
        stats['news_collected'] = news_stats['total_news']
        stats['errors'].extend(news_stats['errors'])
        
        logger.info(f"âœ… {news_stats['total_news']}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ ({news_stats['failed_tickers']}ê°œ ì¢…ëª© ì‹¤íŒ¨)")
        
        # ========== ë‹¨ê³„ 3: LLM ìš”ì•½ ìƒì„± ==========
        logger.info("[3/4] LLM ìš”ì•½ ìƒì„± ì¤‘...")
        summary_stats = generate_summaries(today_str, candidates, db)
        stats['summaries_generated'] = summary_stats['success_count']
        stats['errors'].extend(summary_stats['errors'])
        
        logger.info(f"âœ… {summary_stats['success_count']}ê°œ ìš”ì•½ ìƒì„± ì™„ë£Œ ({summary_stats['failed_count']}ê°œ ì‹¤íŒ¨)")
        
        # ========== ë‹¨ê³„ 4: Google Sheets ë°±ì—… (ì˜µì…˜) ==========
        if study_mode in ['gsheet', 'both']:
            logger.info("[4/4] Google Sheets ë°±ì—… ì¤‘...")
            try:
                backup_to_gsheet(today_str, db, notifier)
                logger.info("âœ… Google Sheets ë°±ì—… ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"Google Sheets ë°±ì—… ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")
                stats['errors'].append(f"GSheet backup failed: {e}")
        else:
            logger.info("[4/4] Google Sheets ë°±ì—… ê±´ë„ˆëœ€ (STUDY_MODE={study_mode})")
        
        # Run ì„±ê³µ ì¢…ë£Œ
        final_status = 'success' if not stats['errors'] else 'partial'
        db.end_run(today_str, final_status, stats=stats)
        
        # ì™„ë£Œ ì•Œë¦¼
        send_completion_notification(today_str, stats, notifier, db)
        
        logger.info("=" * 80)
        logger.info(f"ìœ ëª©ë¯¼ ê³µë¶€ë²• ì™„ë£Œ: {final_status}")
        logger.info("=" * 80)
    
    except Exception as e:
        logger.error(f"ìœ ëª©ë¯¼ ê³µë¶€ë²• ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", exc_info=True)
        db.end_run(today_str, 'fail', error_message=str(e), stats=stats)
        notifier.send_alert(f"âŒ ìœ ëª©ë¯¼ ê³µë¶€ë²• ì‹¤íŒ¨: {e}", level='error')


# ==================== ë‹¨ê³„ë³„ í•¨ìˆ˜ ====================

def collect_market_data(run_date: str, db: StudyDatabase) -> List[Dict]:
    """
    ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ë° í›„ë³´ ì¢…ëª© í•„í„°ë§
    
    Returns:
        í›„ë³´ ì¢…ëª© ë¦¬ìŠ¤íŠ¸
    """
    candidates = []
    
    try:
        # pykrxë¡œ ì „ì²´ ì¢…ëª© ì¡°íšŒ
        df_all = stock.get_market_ohlcv_by_ticker(run_date, market="ALL")
        
        if df_all.empty:
            logger.warning("No market data available for today")
            return candidates
        
        # í•„í„°: ê±°ë˜ëŸ‰ ì²œë§Œì£¼ OR ìƒí•œê°€(29%+)
        volume_filter = df_all['ê±°ë˜ëŸ‰'] >= 10_000_000
        price_ceil_filter = df_all['ë“±ë½ë¥ '] >= 29.0
        interesting_df = df_all[volume_filter | price_ceil_filter]
        
        if interesting_df.empty:
            logger.info("No stocks met the criteria")
            return candidates
        
        # ETF, ìŠ¤íŒ© ë“± ì œì™¸
        unfiltered_tickers = interesting_df.index.tolist()
        eligible_tickers = [
            ticker for ticker in unfiltered_tickers
            if is_eligible_stock(stock.get_market_ticker_name(ticker))
        ]
        
        if not eligible_tickers:
            logger.info("No eligible stocks after filtering")
            return candidates
        
        # ê±°ë˜ëŒ€ê¸ˆ ì¡°íšŒ (ì˜µì…˜)
        try:
            df_trading_value = stock.get_market_trading_value_by_ticker(run_date, market="ALL")
        except:
            df_trading_value = None
        
        # í›„ë³´ ì¢…ëª© ì •ë³´ êµ¬ì„±
        for ticker in eligible_tickers:
            try:
                stock_info = interesting_df.loc[ticker]
                stock_name = stock.get_market_ticker_name(ticker)
                
                # ì‹œì¥ êµ¬ë¶„ (KOSPI/KOSDAQ)
                market = stock.get_market_ticker_list(run_date, market="KOSPI")
                market_type = "KOSPI" if ticker in market else "KOSDAQ"
                
                # ì„ ì • ì‚¬ìœ 
                reasons = []
                if stock_info['ë“±ë½ë¥ '] >= 29.0:
                    reasons.append('limit_up')
                if stock_info['ê±°ë˜ëŸ‰'] >= 10_000_000:
                    reasons.append('volume_10m')
                reason_flag = ' / '.join(reasons) if reasons else 'both'
                
                # ê±°ë˜ëŒ€ê¸ˆ
                value_traded = None
                if df_trading_value is not None and ticker in df_trading_value.index:
                    value_traded = int(df_trading_value.loc[ticker, 'ê±°ë˜ëŒ€ê¸ˆ'])
                
                candidate = {
                    'run_date': run_date,
                    'ticker': ticker,
                    'name': stock_name,
                    'market': market_type,
                    'close_price': int(stock_info['ì¢…ê°€']),
                    'change_pct': float(stock_info['ë“±ë½ë¥ ']),
                    'volume': int(stock_info['ê±°ë˜ëŸ‰']),
                    'value_traded': value_traded,
                    'reason_flag': reason_flag
                }
                
                candidates.append(candidate)
            
            except Exception as e:
                logger.error(f"Failed to process ticker {ticker}: {e}")
                continue
        
        # DBì— ì¼ê´„ ì €ì¥
        if candidates:
            db.insert_candidates(candidates)
            logger.info(f"Inserted {len(candidates)} candidates into database")
    
    except Exception as e:
        logger.error(f"Market data collection failed: {e}", exc_info=True)
        raise  # ì‹œì¥ ë°ì´í„° ì‹¤íŒ¨ëŠ” ì „ì²´ run ì¤‘ë‹¨
    
    return candidates


def collect_news_for_candidates(run_date: str, candidates: List[Dict], 
                                 db: StudyDatabase) -> Dict:
    """
    í›„ë³´ ì¢…ëª©ë“¤ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘
    
    Returns:
        {'total_news': int, 'failed_tickers': int, 'errors': []}
    """
    news_provider = NaverNewsProvider(max_items_per_ticker=20)
    
    total_news = 0
    failed_tickers = 0
    errors = []
    
    for candidate in candidates:
        ticker = candidate['ticker']
        stock_name = candidate['name']
        
        try:
            logger.info(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘: {stock_name} ({ticker})")
            
            # ë‰´ìŠ¤ ìˆ˜ì§‘
            news_items = news_provider.fetch_news(ticker, stock_name, run_date)
            
            if news_items:
                # run_date ë° ticker ì¶”ê°€
                for item in news_items:
                    item['run_date'] = run_date
                    item['ticker'] = ticker
                
                # DB ì €ì¥
                db.insert_news_items(news_items)
                total_news += len(news_items)
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                db.update_candidate_status(run_date, ticker, 'news_collected')
                logger.debug(f"âœ“ {ticker}: {len(news_items)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
            else:
                logger.warning(f"âœ— {ticker}: ë‰´ìŠ¤ ì—†ìŒ")
                db.update_candidate_status(run_date, ticker, 'no_news')
            
            # Rate limiting
            time.sleep(0.3)
        
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {ticker} - {e}")
            db.update_candidate_status(run_date, ticker, 'news_failed')
            failed_tickers += 1
            errors.append(f"News collection failed for {ticker}: {e}")
    
    return {
        'total_news': total_news,
        'failed_tickers': failed_tickers,
        'errors': errors
    }


def generate_summaries(run_date: str, candidates: List[Dict], 
                      db: StudyDatabase) -> Dict:
    """
    LLMìœ¼ë¡œ ì¢…ëª© ìš”ì•½ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
    
    Returns:
        {'success_count': int, 'failed_count': int, 'errors': []}
    """
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        logger.warning("GEMINI_API_KEY not found. Skipping summaries.")
        return {'success_count': 0, 'failed_count': 0, 'errors': ['No API key']}
    
    success_count = 0
    failed_count = 0
    errors = []
    
    # ìš”ì•½ì´ í•„ìš”í•œ ì¢…ëª©ë§Œ í•„í„°ë§ (ìºì‹±)
    stocks_to_summarize = []
    for candidate in candidates:
        ticker = candidate['ticker']
        
        # ì´ë¯¸ ìš”ì•½ì´ ìˆëŠ”ì§€ í™•ì¸
        if db.has_summary(run_date, ticker):
            logger.debug(f"Summary already exists for {ticker}, skipping")
            continue
        
        stocks_to_summarize.append({
            'ticker': ticker,
            'name': candidate['name']
        })
    
    if not stocks_to_summarize:
        logger.info("No new summaries needed (all cached)")
        return {'success_count': 0, 'failed_count': 0, 'errors': []}
    
    # Gemini API ì„¤ì •
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash-exp')
        
        # ë°°ì¹˜ í¬ê¸° ì„¤ì •
        batch_size = int(os.getenv('LLM_BATCH_SIZE', '10'))
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(stocks_to_summarize), batch_size):
            batch = stocks_to_summarize[i:i + batch_size]
            
            logger.info(f"ë°°ì¹˜ ìš”ì•½ ìƒì„± ì¤‘ ({i+1}-{i+len(batch)}/{len(stocks_to_summarize)})")
            
            try:
                summaries = get_batch_summaries_gemini(batch, model, run_date, db)
                
                for ticker, summary_data in summaries.items():
                    if summary_data['success']:
                        success_count += 1
                        db.update_candidate_status(run_date, ticker, 'summarized')
                    else:
                        failed_count += 1
                        errors.append(f"Summary failed for {ticker}")
                
                # Rate limiting
                time.sleep(2)
            
            except Exception as e:
                logger.error(f"Batch summary failed: {e}")
                failed_count += len(batch)
                errors.append(f"Batch summary error: {e}")
    
    except Exception as e:
        logger.error(f"Gemini API setup failed: {e}", exc_info=True)
        errors.append(f"Gemini setup failed: {e}")
    
    return {
        'success_count': success_count,
        'failed_count': failed_count,
        'errors': errors
    }


def get_batch_summaries_gemini(stocks: List[Dict], model, run_date: str, 
                               db: StudyDatabase) -> Dict:
    """
    Gemini APIë¡œ ë°°ì¹˜ ìš”ì•½ ìƒì„±
    
    Returns:
        {ticker: {'success': bool, 'summary': str}, ...}
    """
    results = {}
    
    try:
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        stock_list_str = "\n".join([f"- {s['name']} ({s['ticker']})" for s in stocks])
        
        prompt = (
            "ì•„ë˜ ì£¼ì‹ ì¢…ëª©ë“¤ì— ëŒ€í•´, ê°ê°ì„ **í•œêµ­ì–´ë¡œ 3~5ë¬¸ì¥**ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.\n"
            "ê° ì¢…ëª©ë§ˆë‹¤:\n"
            "1) í•µì‹¬ ì‚¬ì—… ë¶„ì•¼\n"
            "2) ìµœê·¼ ì£¼ê°€ ìƒìŠ¹/ì£¼ëª©ë°›ëŠ” ì´ìœ  (ìˆë‹¤ë©´)\n"
            "3) ì£¼ìš” ê³ ê°ì‚¬ ë˜ëŠ” ê²½ìŸë ¥\n\n"
            "ê²°ê³¼ëŠ” **ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹**ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì¤˜:\n"
            "```json\n"
            "{\n"
            '  "ì¢…ëª©ì½”ë“œ": "ìš”ì•½ ë‚´ìš© (ì¤„ë°”ê¿ˆ í¬í•¨)",\n'
            "  ...\n"
            "}\n"
            "```\n\n"
            f"ìš”ì•½í•  ì¢…ëª©:\n{stock_list_str}"
        )
        
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        
        # JSON íŒŒì‹±
        response_text = response_text.replace("```json", "").replace("```", "").strip()
        start_idx = response_text.find('{')
        end_idx = response_text.rfind('}')
        
        if start_idx != -1 and end_idx != -1:
            json_text = response_text[start_idx:end_idx+1]
            json_response = json.loads(json_text)
            
            # DBì— ì €ì¥
            for ticker, summary_text in json_response.items():
                try:
                    db.insert_summary({
                        'run_date': run_date,
                        'ticker': ticker,
                        'summary_text': summary_text,
                        'llm_provider': 'gemini',
                        'llm_model': 'gemini-2.0-flash-exp'
                    })
                    
                    results[ticker] = {'success': True, 'summary': summary_text}
                
                except Exception as e:
                    logger.error(f"Failed to save summary for {ticker}: {e}")
                    results[ticker] = {'success': False}
        else:
            logger.warning("Gemini ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            for stock in stocks:
                results[stock['ticker']] = {'success': False}
    
    except Exception as e:
        logger.error(f"Batch summary generation failed: {e}", exc_info=True)
        for stock in stocks:
            results[stock['ticker']] = {'success': False}
    
    return results


def backup_to_gsheet(run_date: str, db: StudyDatabase, notifier):
    """Google Sheets ë°±ì—… (ì˜µì…˜)"""
    try:
        from .study_legacy import get_gsheet_client, get_worksheet_or_create
        from gspread_dataframe import set_with_dataframe
        import pandas as pd
        
        # ë°ì´í„° ì¡°íšŒ
        data = db.get_full_study_data(run_date)
        candidates = data['candidates']
        summaries = data['summaries']
        
        if not candidates:
            return
        
        # DataFrame êµ¬ì„±
        records = []
        for candidate in candidates:
            ticker = candidate['ticker']
            summary = summaries.get(ticker, {})
            
            records.append({
                'ë‚ ì§œ': run_date,
                'ì¢…ëª©ì½”ë“œ': ticker,
                'ì¢…ëª©ëª…': candidate['name'],
                'ì„ ì •ì‚¬ìœ ': candidate['reason_flag'],
                'ì¢…ê°€': f"{candidate['close_price']:,}",
                'ë“±ë½ë¥ ': f"{candidate['change_pct']:.2f}%",
                'ê±°ë˜ëŸ‰': f"{candidate['volume']:,}",
                'ê¸°ì—…ê°œìš”': summary.get('summary_text', 'ìš”ì•½ ì—†ìŒ')
            })
        
        # Google Sheets ì—…ë°ì´íŠ¸
        gsheet_client = get_gsheet_client()
        spreadsheet = gsheet_client.open("ì‹œì¥ ê´€ì‹¬ì£¼ ì¶”ì ")
        log_ws = get_worksheet_or_create(spreadsheet, "DailyLog")
        
        # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
        existing_df = pd.DataFrame(log_ws.get_all_records())
        new_df = pd.DataFrame(records).astype(str)
        
        if not existing_df.empty:
            combined_df = pd.concat([existing_df, new_df], ignore_index=True)
        else:
            combined_df = new_df
        
        set_with_dataframe(log_ws, combined_df, include_index=False, resize=True)
        logger.info(f"Backed up {len(records)} records to Google Sheets")
    
    except Exception as e:
        raise Exception(f"GSheet backup failed: {e}")


def send_completion_notification(run_date: str, stats: Dict, notifier, db: StudyDatabase):
    """ì™„ë£Œ ì•Œë¦¼ ë°œì†¡"""
    try:
        # ìƒìœ„ 5ê°œ ì¢…ëª© ì •ë³´
        candidates = db.get_candidates(run_date)[:5]
        
        fields = []
        for candidate in candidates:
            fields.append({
                "name": f"ğŸ“Š {candidate['name']} ({candidate['ticker']})",
                "value": f"ë“±ë½ë¥ : {candidate['change_pct']:.2f}% | ì‚¬ìœ : {candidate['reason_flag']}",
                "inline": False
            })
        
        embed = {
            "title": f"ğŸ“š ìœ ëª©ë¯¼ ê³µë¶€ë²• ì™„ë£Œ ({run_date})",
            "description": (
                f"âœ… í›„ë³´ ì¢…ëª©: **{stats['candidates']}ê°œ**\n"
                f"ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘: **{stats['news_collected']}ê°œ**\n"
                f"ğŸ¤– AI ìš”ì•½: **{stats['summaries_generated']}ê°œ**\n"
                f"âš ï¸ ì˜¤ë¥˜: **{len(stats['errors'])}ê±´**"
            ),
            "color": 5814783,
            "fields": fields,
            "footer": {"text": f"SQLite DB ì €ì¥ ì™„ë£Œ | ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸ ê°€ëŠ¥"}
        }
        
        notifier.send_alert("ìœ ëª©ë¯¼ ê³µë¶€ë²• ì™„ë£Œ", embed=embed)
    
    except Exception as e:
        logger.error(f"Failed to send notification: {e}")


# ==================== CLI ì¸í„°í˜ì´ìŠ¤ ====================

if __name__ == '__main__':
    import argparse
    from .notifier import Notifier
    
    parser = argparse.ArgumentParser(description='ìœ ëª©ë¯¼ ê³µë¶€ë²• ìˆ˜ë™ ì‹¤í–‰')
    parser.add_argument('--force', action='store_true', help='ê°•ì œ ì‹¤í–‰ (ì¤‘ë³µ ë¬´ì‹œ)')
    parser.add_argument('--date', type=str, help='íŠ¹ì • ë‚ ì§œ ì‹¤í–‰ (YYYYMMDD)')
    parser.add_argument('--news-only', action='store_true', help='ë‰´ìŠ¤ë§Œ ì¬ìˆ˜ì§‘')
    
    args = parser.parse_args()
    
    # Notifier ì´ˆê¸°í™”
    notifier = Notifier()
    
    if args.date:
        # íŠ¹ì • ë‚ ì§œë¡œ ì‹¤í–‰ (ë¯¸êµ¬í˜„ - í–¥í›„ í™•ì¥ ê°€ëŠ¥)
        print(f"íŠ¹ì • ë‚ ì§œ ì‹¤í–‰ ê¸°ëŠ¥ì€ í–¥í›„ êµ¬í˜„ ì˜ˆì •: {args.date}")
    else:
        # ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì‹¤í–‰
        run_daily_study(None, notifier, force_run=args.force)
