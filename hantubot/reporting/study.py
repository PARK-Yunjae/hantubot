# hantubot_prod/hantubot/reporting/study.py
import os
import time
from datetime import datetime
from typing import List, Dict
import json

import gspread
import pandas as pd
from google.oauth2.service_account import Credentials
from gspread_dataframe import set_with_dataframe
from pykrx import stock
import google.generativeai as genai  # ì•ˆì • ë²„ì „ ì‚¬ìš©

from .logger import get_logger

logger = get_logger(__name__)

# --- Configuration ---
GSHEET_SCOPE = [
    'https://www.googleapis.com/auth/spreadsheets',
    'https://www.googleapis.com/auth/drive'
]
GSHEET_CONFIG_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'configs', 'google_service_account.json')
GSHEET_NAME = "ì‹œì¥ ê´€ì‹¬ì£¼ ì¶”ì "

# --- Gemini API Functions (Batch Optimized) ---
def get_batch_summaries_with_gemini(stocks_to_summarize: List[Dict]) -> Dict[str, str]:
    """
    Uses the Gemini API to generate concise summaries for a batch of stocks in a single call.
    Returns a dictionary mapping ticker to summary.
    """
    summaries = {stock['ticker']: "ìš”ì•½ ìƒì„± ì‹¤íŒ¨" for stock in stocks_to_summarize}
    
    try:
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            logger.warning("GEMINI_API_KEY not found in .env file. Skipping summary.")
            return summaries

        genai.configure(api_key=api_key)
        # ë¬´ë£Œ í‹°ì–´ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì‹  ì•ˆì • ëª¨ë¸
        model = genai.GenerativeModel('gemini-2.0-flash')
        
        # Build a single prompt for all stocks
        stock_list_str = "\n".join([f"- {s['name']} ({s['ticker']})" for s in stocks_to_summarize])
        prompt = (
            "ì•„ë˜ ì£¼ì‹ ì¢…ëª©ë“¤ì— ëŒ€í•´, ê°ê°ì˜ í•µì‹¬ ì‚¬ì—… ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ 2~3 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.\n"
            "ê° ë¬¸ì¥ ëì—ëŠ” ì¤„ë°”ê¿ˆ ë¬¸ì(\\n)ë¥¼ í¬í•¨í•´ì„œ ê°€ë…ì„±ì„ ë†’ì—¬ì¤˜.\n"
            "ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œ 'ì¢…ëª©ì½”ë“œ': 'ìš”ì•½' í˜•íƒœë¡œ ì œê³µí•´ì¤˜. ë‹¤ë¥¸ ì„¤ëª…ì€ ëª¨ë‘ ì œì™¸í•´ì¤˜.\n"
            "```json\n"
            "{\n"
            '  "005930": "ì„¸ê³„ì ì¸ ì¢…í•© ë°˜ë„ì²´ ê¸°ì—…ìœ¼ë¡œ, ë©”ëª¨ë¦¬ ë°˜ë„ì²´ì™€ ì‹œìŠ¤í…œ LSI ì‚¬ì—…ì„ ì˜ìœ„í•¨.\nìŠ¤ë§ˆíŠ¸í°, TV, ê°€ì „ì œí’ˆ ë“± ë‹¤ì–‘í•œ ì „ìì œí’ˆì„ ìƒì‚° ë° íŒë§¤í•˜ë©° ê¸€ë¡œë²Œ IT ì‹œì¥ì„ ì„ ë„í•¨.",\n'
            '  "000660": "DRAM, ë‚¸ë“œí”Œë˜ì‹œ ë“± ë©”ëª¨ë¦¬ ë°˜ë„ì²´ë¥¼ ì£¼ë ¥ìœ¼ë¡œ ìƒì‚°í•˜ëŠ” ê¸°ì—…ì„.\nì„œë²„, ëª¨ë°”ì¼, PC ë“± ë‹¤ì–‘í•œ IT ê¸°ê¸°ì— í•„ìˆ˜ì ì¸ ë¶€í’ˆì„ ê³µê¸‰í•˜ë©° ê¸°ìˆ  ê²½ìŸë ¥ì„ í™•ë³´í•˜ê³  ìˆìŒ."\n'
            "}\n"
            "```\n\n"
            f"ìš”ì•½í•  ì¢…ëª© ëª©ë¡:\n{stock_list_str}"
        )
        
        response = model.generate_content(prompt)
        
        # Clean up and parse the JSON response
        response_text = response.text.strip()
        
        # Remove markdown code blocks
        response_text = response_text.replace("```json", "").replace("```", "").strip()
        
        # Find JSON content (sometimes Gemini adds extra text)
        start_idx = response_text.find('{')
        end_idx = response_text.rfind('}')
        
        if start_idx != -1 and end_idx != -1:
            json_text = response_text[start_idx:end_idx+1]
            json_response = json.loads(json_text)
            
            # Gemini might return summaries for different tickers, so we update our dict safely
            for ticker, summary in json_response.items():
                if ticker in summaries:
                    summaries[ticker] = summary
            
            logger.info(f"Successfully generated summaries for {len(json_response)} stocks in a single batch call.")
        else:
            logger.warning("Gemini ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return summaries

    except Exception as e:
        logger.error(f"Failed to get batch company summaries using Gemini API: {e}", exc_info=True)
        return summaries


# --- Google Sheets Functions ---
def get_gsheet_client():
    """Authenticate with Google and return the gspread client."""
    if not os.path.exists(GSHEET_CONFIG_PATH):
        raise FileNotFoundError(f"Google Service Account key not found at {GSHEET_CONFIG_PATH}")
    creds = Credentials.from_service_account_file(GSHEET_CONFIG_PATH, scopes=GSHEET_SCOPE)
    return gspread.authorize(creds)

def get_worksheet_or_create(spreadsheet: gspread.Spreadsheet, name: str):
    """Get a worksheet by name, or create it if it doesn't exist."""
    try:
        return spreadsheet.worksheet(name)
    except gspread.WorksheetNotFound:
        logger.info(f"Worksheet '{name}' not found, creating it.")
        return spreadsheet.add_worksheet(title=name, rows=1, cols=1)

# --- Main Study Logic ---
def run_daily_study(broker, notifier, force_run=False):
    """
    "100ì¼ ê³µë¶€" ë¦¬ì„œì¹˜ ë£¨í‹´: Google Sheets & Gemini API ì™„ì „ ìë™í™” ë²„ì „
    
    Args:
        broker: ë¸Œë¡œì»¤ ì¸ìŠ¤í„´ìŠ¤
        notifier: ì•Œë¦¼ ì¸ìŠ¤í„´ìŠ¤
        force_run: Trueë©´ ì¤‘ë³µ ì²´í¬ ë¬´ì‹œí•˜ê³  ê°•ì œ ì‹¤í–‰
    """
    logger.info("Running daily study: Fully Automated GSheet + Gemini Edition...")
    today_str = datetime.now().strftime("%Y%m%d")
    today_date_str_for_check = datetime.now().strftime("%Y-%m-%d")

    # 1. Connect to Google Sheets and check for duplicates FIRST
    try:
        gsheet_client = get_gsheet_client()
        spreadsheet = gsheet_client.open(GSHEET_NAME)
        log_ws = get_worksheet_or_create(spreadsheet, "DailyLog")
        
        existing_df = pd.DataFrame(log_ws.get_all_records())
        if not force_run and not existing_df.empty and today_date_str_for_check in existing_df['ë‚ ì§œ'].values:
            logger.info(f"Today's study for {today_date_str_for_check} has already been completed. Skipping.")
            return

        freq_ws = get_worksheet_or_create(spreadsheet, "Frequency_Analysis")
    except Exception as e:
        logger.error(f"Failed to connect to Google Sheets for pre-check: {e}", exc_info=True)
        notifier.send_alert(f"Google Sheets ì—°ê²° ì‹¤íŒ¨ (ì‚¬ì „ í™•ì¸): {e}", level='error')
        return

    # 2. Fetch interesting stocks from pykrx
    try:
        df_all = stock.get_market_ohlcv_by_ticker(today_str, market="ALL")
        volume_filter = df_all['ê±°ë˜ëŸ‰'] >= 10_000_000
        price_ceil_filter = df_all['ë“±ë½ë¥ '] >= 29.0
        interesting_tickers_df = df_all[volume_filter | price_ceil_filter]
        
        if interesting_tickers_df.empty:
            logger.info("No stocks met the criteria for daily study today.")
            return
        
        # [ìˆ˜ì •] ETF, ìŠ¤íŒ© ë“± ì œì™¸ í•„í„°ë§ ì ìš©
        from ..utils.stock_filters import is_eligible_stock
        
        unfiltered_tickers = interesting_tickers_df.index.tolist()
        interesting_tickers = [
            ticker for ticker in unfiltered_tickers 
            if is_eligible_stock(stock.get_market_ticker_name(ticker))
        ]
        
        if not interesting_tickers:
            logger.info("í•„í„°ë§ëœ ì¢…ëª©ì´ ì—†ì–´ ë°ì¼ë¦¬ ìŠ¤í„°ë”” ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        logger.info(f"í•„í„°ë§ í›„ ë°ì¼ë¦¬ ìŠ¤í„°ë”” ëŒ€ìƒ ì ê²© ì¢…ëª© {len(interesting_tickers)}ê°œ ë°œê²¬.")
        df_funda = stock.get_market_fundamental_by_ticker(today_str)
    except Exception as e:
        logger.error(f"Failed to fetch stocks for daily study from pykrx: {e}", exc_info=True)
        return

    # 3. Get all summaries in one batch call
    stocks_to_summarize = [{'ticker': t, 'name': stock.get_market_ticker_name(t)} for t in interesting_tickers]
    all_summaries = get_batch_summaries_with_gemini(stocks_to_summarize)
    time.sleep(15) # Respect potential API rate limits after a large call

    # 4. Process each stock and gather data
    daily_records = []
    for ticker in interesting_tickers:
        try:
            stock_info = interesting_tickers_df.loc[ticker]
            stock_name = stock.get_market_ticker_name(ticker)
            
            company_summary = all_summaries.get(ticker, "ìš”ì•½ ì—†ìŒ.")
            
            reason = ", ".join([r for r, c in [("ê±°ë˜ëŸ‰ì²œë§Œ", stock_info['ê±°ë˜ëŸ‰'] >= 10_000_000), ("ìƒí•œê°€", stock_info['ë“±ë½ë¥ '] >= 29.0)] if c])

            # ê°„ì†Œí™”ëœ ì»¬ëŸ¼ (ì¬ë¬´ì§€í‘œ ì œì™¸)
            daily_records.append({
                "ë‚ ì§œ": today_date_str_for_check,
                "ì¢…ëª©ì½”ë“œ": ticker,
                "ì¢…ëª©ëª…": stock_name,
                "ì„ ì •ì‚¬ìœ ": reason,
                "ì¢…ê°€": f"{stock_info['ì¢…ê°€']:,}",
                "ë“±ë½ë¥ ": f"{stock_info['ë“±ë½ë¥ ']:.2f}%",
                "ê±°ë˜ëŸ‰": f"{stock_info['ê±°ë˜ëŸ‰']:,}",
                "ê¸°ì—…ê°œìš”": company_summary,
            })
        except Exception as e:
            logger.error(f"Failed to process {ticker} for GSheet: {e}")

    if not daily_records:
        logger.info("No records to update to Google Sheets.")
        return
        
    # 5. Update Google Sheets
    try:
        new_df = pd.DataFrame(daily_records)
        if not existing_df.empty:
            combined_df = pd.concat([existing_df, new_df], ignore_index=True)
        else:
            combined_df = new_df
        
        # Ensure all data is string to avoid gspread issues
        combined_df = combined_df.astype(str)

        set_with_dataframe(log_ws, combined_df, include_index=False, resize=True)
        logger.info(f"Appended {len(new_df)} new records to 'DailyLog' worksheet.")
        
        # ìë™ ì—´ ë„ˆë¹„ ì¡°ì • (ë‚´ìš©ì— ë§ê²Œ)
        try:
            # ëª¨ë“  ì—´ì— ëŒ€í•´ ìë™ í¬ê¸° ì¡°ì • ìš”ì²­
            num_cols = len(combined_df.columns)
            log_ws.columns_auto_resize(0, num_cols - 1)
            logger.info("ì—´ ë„ˆë¹„ ìë™ ì¡°ì • ì™„ë£Œ.")
        except Exception as e:
            logger.warning(f"ì—´ ë„ˆë¹„ ìë™ ì¡°ì • ì‹¤íŒ¨ (ë¬´ì‹œ ê°€ëŠ¥): {e}")

        # Update Frequency Analysis using Korean column name
        freq_counts = combined_df['ì¢…ëª©ëª…'].value_counts().reset_index()
        freq_counts.columns = ['ì¢…ëª©ëª…', 'ë“±ì¥íšŸìˆ˜']
        set_with_dataframe(freq_ws, freq_counts, include_index=False, resize=True)
        logger.info("Updated 'Frequency_Analysis' worksheet.")
        
        # Frequency ì‹œíŠ¸ë„ ìë™ í¬ê¸° ì¡°ì •
        try:
            freq_ws.columns_auto_resize(0, 1)
            logger.info("Frequency ì‹œíŠ¸ ì—´ ë„ˆë¹„ ìë™ ì¡°ì • ì™„ë£Œ.")
        except Exception as e:
            logger.warning(f"Frequency ì‹œíŠ¸ ì—´ ë„ˆë¹„ ìë™ ì¡°ì • ì‹¤íŒ¨ (ë¬´ì‹œ ê°€ëŠ¥): {e}")

        summary_fields = [{"name": f"- {rec['ì¢…ëª©ëª…']} ({rec['ì¢…ëª©ì½”ë“œ']})", "value": f"ì´ìœ : {rec['ì„ ì •ì‚¬ìœ ']}", "inline": False} for rec in daily_records[:5]]
        embed = {"title": f"ğŸ“ ìœ ëª©ë¯¼ ê³µë¶€ë²• ë¦¬í¬íŠ¸ -> GSheet ì €ì¥ ì™„ë£Œ", "description": f"ê¸ˆì¼ì˜ ê´€ì‹¬ ì¢…ëª© **{len(daily_records)}ê°œ**ê°€ ìë™ ìš”ì•½ê³¼ í•¨ê»˜ Google Sheetì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.", "color": 5814783, "fields": summary_fields}
        notifier.send_alert("ìœ ëª©ë¯¼ ê³µë¶€ë²• ë¶„ì„ ì™„ë£Œ", embed=embed)

    except Exception as e:
        logger.error(f"Failed to update Google Sheets: {e}", exc_info=True)
        notifier.send_alert(f"Google Sheets ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}", level='error')

if __name__ == '__main__':
    # This test won't work without a mock for gspread and gemini
    pass
