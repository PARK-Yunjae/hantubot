# hantubot_prod/hantubot/reporting/study.py
"""
ìœ ëª©ë¯¼ ê³µë¶€ë²• (100ì¼ ê³µë¶€) - SQLite + ë‰´ìŠ¤ ìˆ˜ì§‘ + LLM ìš”ì•½ í†µí•© ë²„ì „
"""
import os
import time
import json
from datetime import datetime
from typing import List, Dict, Optional

from pykrx import stock
import google.generativeai as genai

from .logger import get_logger
from .study_db import get_study_db, StudyDatabase
from ..utils.stock_filters import is_eligible_stock
from ..providers import NaverNewsProvider
from ..utils.config_loader import load_config

logger = get_logger(__name__)


def backup_database():
    """
    study.db ìë™ ë°±ì—… (config.yaml ì„¤ì • ê¸°ë°˜)
    
    - ì¼ìš”ì¼ë§ˆë‹¤ ìë™ ë°±ì—…
    - ì„¤ì •ëœ ê¸°ê°„(ê¸°ë³¸ 30ì¼) ì´ìƒ ëœ ë°±ì—… ìë™ ì‚­ì œ
    """
    try:
        from pathlib import Path
        from datetime import datetime
        import shutil
        
        config = load_config()
        study_config = config.get('study', {})
        
        # ë°±ì—… í™œì„±í™” ì²´í¬
        if not study_config.get('enable_auto_backup', True):
            logger.debug("DB ìë™ ë°±ì—…ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        db_path = Path('data/study.db')
        if not db_path.exists():
            logger.warning("study.db íŒŒì¼ì´ ì—†ì–´ ë°±ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        backup_dir = Path('data/backups')
        backup_dir.mkdir(exist_ok=True)
        
        # ì¼ìš”ì¼(weekday=6)ë§ˆë‹¤ ë°±ì—…
        now = datetime.now()
        if now.weekday() == 6:
            backup_file = backup_dir / f"study_backup_{now:%Y%m%d}.db"
            
            # ì´ë¯¸ ì˜¤ëŠ˜ ë°±ì—…ì´ ìˆìœ¼ë©´ ê±´ë„ˆëœ€
            if backup_file.exists():
                logger.info(f"ì˜¤ëŠ˜ ë°±ì—…ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {backup_file}")
                return
            
            shutil.copy(db_path, backup_file)
            logger.info(f"âœ… DB ë°±ì—… ì™„ë£Œ: {backup_file}")
            
            # ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ
            retention_days = study_config.get('backup_retention_days', 30)
            deleted_count = 0
            for old_backup in backup_dir.glob("study_backup_*.db"):
                age_days = (now - datetime.fromtimestamp(old_backup.stat().st_mtime)).days
                if age_days > retention_days:
                    old_backup.unlink()
                    deleted_count += 1
                    logger.info(f"ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {old_backup} ({age_days}ì¼)")
            
            if deleted_count > 0:
                logger.info(f"ì´ {deleted_count}ê°œì˜ ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ ì™„ë£Œ")
        else:
            logger.debug(f"ë°±ì—… ì˜ˆì •ì¼ì´ ì•„ë‹™ë‹ˆë‹¤. (í˜„ì¬: {now.strftime('%A')}, ë°±ì—…ì¼: ì¼ìš”ì¼)")
    
    except Exception as e:
        logger.error(f"DB ë°±ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)


def get_latest_trading_date() -> str:
    """
    ìµœê·¼ ê±°ë˜ì¼ì„ ì¡°íšŒí•©ë‹ˆë‹¤ (ì˜¤ëŠ˜ì´ íœ´ì¥ì¼ì´ë©´ ì´ì „ ê±°ë˜ì¼ ë°˜í™˜)
    
    Returns:
        YYYYMMDD í˜•ì‹ì˜ ìµœê·¼ ê±°ë˜ì¼
    """
    from datetime import datetime, timedelta
    
    today = datetime.now()
    
    # ìµœëŒ€ 10ì¼ ì „ê¹Œì§€ í™•ì¸ (ì£¼ë§, ê³µíœ´ì¼ ê³ ë ¤)
    for i in range(10):
        check_date = today - timedelta(days=i)
        date_str = check_date.strftime("%Y%m%d")
        
        try:
            # pykrxë¡œ í•´ë‹¹ ë‚ ì§œì— ì‹œì¥ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            df = stock.get_market_ohlcv_by_ticker(date_str, market="KOSPI")
            if not df.empty:
                logger.info(f"ìµœê·¼ ê±°ë˜ì¼ í™•ì¸: {date_str}")
                return date_str
        except:
            continue
    
    # ì°¾ì§€ ëª»í•˜ë©´ ì˜¤ëŠ˜ ë‚ ì§œ ë°˜í™˜ (fallback)
    return today.strftime("%Y%m%d")


def run_daily_study(broker, notifier, force_run=False, target_date=None):
    """
    ìœ ëª©ë¯¼ ê³µë¶€ë²• ë©”ì¸ í•¨ìˆ˜ - SQLite ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„
    
    Args:
        broker: ë¸Œë¡œì»¤ ì¸ìŠ¤í„´ìŠ¤ (ë¯¸ì‚¬ìš©, ì‹œê·¸ë‹ˆì²˜ í˜¸í™˜ì„± ìœ ì§€)
        notifier: ì•Œë¦¼ ì¸ìŠ¤í„´ìŠ¤
        force_run: Trueë©´ ì¤‘ë³µ ì²´í¬ ë¬´ì‹œí•˜ê³  ê°•ì œ ì‹¤í–‰
        target_date: íŠ¹ì • ë‚ ì§œ ì§€ì • (YYYYMMDD), Noneì´ë©´ ìµœê·¼ ê±°ë˜ì¼ ìë™ ì¡°íšŒ
    """
    logger.info("=" * 80)
    logger.info("ìœ ëª©ë¯¼ ê³µë¶€ë²• (100ì¼ ê³µë¶€) ì‹œì‘ - SQLite + ë‰´ìŠ¤ ìˆ˜ì§‘ ë²„ì „")
    logger.info("=" * 80)
    
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    study_mode = os.getenv('STUDY_MODE', 'sqlite')  # sqlite / gsheet / both
    
    # ë‚ ì§œ ì„¤ì • (ìµœê·¼ ê±°ë˜ì¼ ìë™ ì¡°íšŒ)
    if target_date:
        today_str = target_date
    else:
        today_str = get_latest_trading_date()
        logger.info(f"ìë™ ì¡°íšŒëœ ìµœê·¼ ê±°ë˜ì¼: {today_str}")
    
    today_date = datetime.strptime(today_str, "%Y%m%d").strftime("%Y-%m-%d")
    
    # DB ì´ˆê¸°í™”
    try:
        db = get_study_db()
    except Exception as e:
        logger.error(f"Failed to initialize database: {e}", exc_info=True)
        notifier.send_alert(f"âŒ ìœ ëª©ë¯¼ ê³µë¶€ë²• DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", level='error')
        return
    
    # 1. ì¤‘ë³µ ì‹¤í–‰ ì²´í¬
    if not force_run:
        existing_run = db.get_run(today_str)
        if existing_run and existing_run['status'] in ['success', 'partial']:
            logger.info(f"Today's study for {today_str} already completed. Skipping.")
            return
    
    # 2. Run ì‹œì‘
    try:
        run_id = db.start_run(today_str)
        logger.info(f"Started new study run: {today_str} (run_id={run_id})")
    except Exception as e:
        logger.error(f"Failed to start run: {e}", exc_info=True)
        notifier.send_alert(f"âŒ ìœ ëª©ë¯¼ ê³µë¶€ë²• ì‹œì‘ ì‹¤íŒ¨: {e}", level='error')
        return
    
    stats = {
        'candidates': 0,
        'news_collected': 0,
        'summaries_generated': 0,
        'errors': []
    }
    
    try:
        # ========== ë‹¨ê³„ 1: ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ==========
        logger.info("[1/4] ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        candidates = collect_market_data(today_str, db)
        stats['candidates'] = len(candidates)
        
        if not candidates:
            logger.info("No candidates found for today. Ending run.")
            db.end_run(today_str, 'success', stats=stats)
            return
        
        logger.info(f"âœ… {len(candidates)}ê°œ í›„ë³´ ì¢…ëª© ë°œê²¬ ë° DB ì €ì¥ ì™„ë£Œ")
        
        # ========== ë‹¨ê³„ 2: ë‰´ìŠ¤ ìˆ˜ì§‘ ==========
        logger.info("[2/4] ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        news_stats = collect_news_for_candidates(today_str, candidates, db)
        stats['news_collected'] = news_stats['total_news']
        stats['errors'].extend(news_stats['errors'])
        
        logger.info(f"âœ… {news_stats['total_news']}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ ({news_stats['failed_tickers']}ê°œ ì¢…ëª© ì‹¤íŒ¨)")
        
        # ========== ë‹¨ê³„ 3: LLM ìš”ì•½ ìƒì„± ==========
        logger.info("[3/4] LLM ìš”ì•½ ìƒì„± ì¤‘...")
        summary_stats = generate_summaries(today_str, candidates, db)
        stats['summaries_generated'] = summary_stats['success_count']
        stats['errors'].extend(summary_stats['errors'])
        
        logger.info(f"âœ… {summary_stats['success_count']}ê°œ ìš”ì•½ ìƒì„± ì™„ë£Œ ({summary_stats['failed_count']}ê°œ ì‹¤íŒ¨)")
        
        # ========== ë‹¨ê³„ 3.5: ë°±ì¼ê³µë¶€ í•™ìŠµ ë©”ëª¨ ìƒì„± (ì„ íƒ) ==========
        enable_study_notes = os.getenv('ENABLE_STUDY_NOTES', 'true').lower() == 'true'
        if enable_study_notes:
            logger.info("[3.5/4] ë°±ì¼ê³µë¶€ í•™ìŠµ ë©”ëª¨ ìƒì„± ì¤‘...")
            note_stats = generate_study_notes(today_str, candidates, db)
            stats['study_notes_generated'] = note_stats['success_count']
            stats['errors'].extend(note_stats['errors'])
            logger.info(f"âœ… {note_stats['success_count']}ê°œ í•™ìŠµ ë©”ëª¨ ìƒì„± ì™„ë£Œ ({note_stats['failed_count']}ê°œ ì‹¤íŒ¨)")
        else:
            logger.info("[3.5/4] ë°±ì¼ê³µë¶€ í•™ìŠµ ë©”ëª¨ ê±´ë„ˆëœ€ (ENABLE_STUDY_NOTES=false)")
        
        # ========== ë‹¨ê³„ 4: Google Sheets ë°±ì—… (ì˜µì…˜) ==========
        if study_mode in ['gsheet', 'both']:
            logger.info("[4/4] Google Sheets ë°±ì—… ì¤‘...")
            try:
                backup_to_gsheet(today_str, db, notifier)
                logger.info("âœ… Google Sheets ë°±ì—… ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"Google Sheets ë°±ì—… ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")
                stats['errors'].append(f"GSheet backup failed: {e}")
        else:
            logger.info("[4/4] Google Sheets ë°±ì—… ê±´ë„ˆëœ€ (STUDY_MODE={study_mode})")
        
        # Run ì„±ê³µ ì¢…ë£Œ
        final_status = 'success' if not stats['errors'] else 'partial'
        db.end_run(today_str, final_status, stats=stats)
        
        # ì™„ë£Œ ì•Œë¦¼
        send_completion_notification(today_str, stats, notifier, db)
        
        # ========== DB ìë™ ë°±ì—… (ì˜µì…˜) ==========
        try:
            logger.info("[ì¶”ê°€] DB ìë™ ë°±ì—… ì²´í¬ ì¤‘...")
            backup_database()
        except Exception as e:
            logger.warning(f"DB ìë™ ë°±ì—… ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
        
        # ========== GitHub ìë™ ì»¤ë°‹ (ì˜µì…˜) ==========
        enable_auto_commit = os.getenv('ENABLE_GIT_AUTO_COMMIT', 'true').lower() == 'true'
        if enable_auto_commit:
            try:
                logger.info("[ì¶”ê°€] GitHub ìë™ ì»¤ë°‹ ì¤‘...")
                auto_commit_to_github(today_str, stats)
                logger.info("âœ… GitHub ì»¤ë°‹ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"GitHub ìë™ ì»¤ë°‹ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")
        
        logger.info("=" * 80)
        logger.info(f"ìœ ëª©ë¯¼ ê³µë¶€ë²• ì™„ë£Œ: {final_status}")
        logger.info("=" * 80)
    
    except Exception as e:
        logger.error(f"ìœ ëª©ë¯¼ ê³µë¶€ë²• ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", exc_info=True)
        db.end_run(today_str, 'fail', error_message=str(e), stats=stats)
        notifier.send_alert(f"âŒ ìœ ëª©ë¯¼ ê³µë¶€ë²• ì‹¤íŒ¨: {e}", level='error')


# ==================== ë‹¨ê³„ë³„ í•¨ìˆ˜ ====================

def collect_market_data(run_date: str, db: StudyDatabase) -> List[Dict]:
    """
    ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ë° í›„ë³´ ì¢…ëª© í•„í„°ë§
    
    Returns:
        í›„ë³´ ì¢…ëª© ë¦¬ìŠ¤íŠ¸
    """
    candidates = []
    
    try:
        # pykrxë¡œ ì „ì²´ ì¢…ëª© ì¡°íšŒ
        df_all = stock.get_market_ohlcv_by_ticker(run_date, market="ALL")
        
        if df_all.empty:
            logger.warning("No market data available for today")
            return candidates
        
        # í•„í„°: ê±°ë˜ëŸ‰ ì²œë§Œì£¼ OR ìƒí•œê°€(29%+)
        volume_filter = df_all['ê±°ë˜ëŸ‰'] >= 10_000_000
        price_ceil_filter = df_all['ë“±ë½ë¥ '] >= 29.0
        interesting_df = df_all[volume_filter | price_ceil_filter]
        
        if interesting_df.empty:
            logger.info("No stocks met the criteria")
            return candidates
        
        # ETF, ìŠ¤íŒ© ë“± ì œì™¸
        unfiltered_tickers = interesting_df.index.tolist()
        eligible_tickers = [
            ticker for ticker in unfiltered_tickers
            if is_eligible_stock(stock.get_market_ticker_name(ticker))
        ]
        
        if not eligible_tickers:
            logger.info("No eligible stocks after filtering")
            return candidates
        
        # ê±°ë˜ëŒ€ê¸ˆ ì¡°íšŒ (ì˜µì…˜)
        try:
            df_trading_value = stock.get_market_trading_value_by_ticker(run_date, market="ALL")
        except:
            df_trading_value = None
        
        # í›„ë³´ ì¢…ëª© ì •ë³´ êµ¬ì„±
        for ticker in eligible_tickers:
            try:
                stock_info = interesting_df.loc[ticker]
                stock_name = stock.get_market_ticker_name(ticker)
                
                # ì‹œì¥ êµ¬ë¶„ (KOSPI/KOSDAQ)
                market = stock.get_market_ticker_list(run_date, market="KOSPI")
                market_type = "KOSPI" if ticker in market else "KOSDAQ"
                
                # ì„ ì • ì‚¬ìœ 
                reasons = []
                if stock_info['ë“±ë½ë¥ '] >= 29.0:
                    reasons.append('limit_up')
                if stock_info['ê±°ë˜ëŸ‰'] >= 10_000_000:
                    reasons.append('volume_10m')
                reason_flag = ' / '.join(reasons) if reasons else 'both'
                
                # ê±°ë˜ëŒ€ê¸ˆ
                value_traded = None
                if df_trading_value is not None and ticker in df_trading_value.index:
                    value_traded = int(df_trading_value.loc[ticker, 'ê±°ë˜ëŒ€ê¸ˆ'])
                
                candidate = {
                    'run_date': run_date,
                    'ticker': ticker,
                    'name': stock_name,
                    'market': market_type,
                    'close_price': int(stock_info['ì¢…ê°€']),
                    'change_pct': float(stock_info['ë“±ë½ë¥ ']),
                    'volume': int(stock_info['ê±°ë˜ëŸ‰']),
                    'value_traded': value_traded,
                    'reason_flag': reason_flag
                }
                
                candidates.append(candidate)
            
            except Exception as e:
                logger.error(f"Failed to process ticker {ticker}: {e}")
                continue
        
        # DBì— ì¼ê´„ ì €ì¥
        if candidates:
            db.insert_candidates(candidates)
            logger.info(f"Inserted {len(candidates)} candidates into database")
    
    except Exception as e:
        logger.error(f"Market data collection failed: {e}", exc_info=True)
        raise  # ì‹œì¥ ë°ì´í„° ì‹¤íŒ¨ëŠ” ì „ì²´ run ì¤‘ë‹¨
    
    return candidates


def collect_news_for_candidates(run_date: str, candidates: List[Dict], 
                                 db: StudyDatabase) -> Dict:
    """
    í›„ë³´ ì¢…ëª©ë“¤ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬ë¡œ 3ë°° ë¹ ë¦„)
    
    Returns:
        {'total_news': int, 'failed_tickers': int, 'errors': []}
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    news_provider = NaverNewsProvider(max_items_per_ticker=20)
    
    total_news = 0
    failed_tickers = 0
    errors = []
    
    def fetch_single_news(candidate):
        """ë‹¨ì¼ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ (ìŠ¤ë ˆë“œ ë‚´ì—ì„œ ì‹¤í–‰)"""
        ticker = candidate['ticker']
        stock_name = candidate['name']
        
        try:
            logger.info(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘: {stock_name} ({ticker})")
            
            # ë‰´ìŠ¤ ìˆ˜ì§‘
            news_items = news_provider.fetch_news(ticker, stock_name, run_date)
            
            if news_items:
                # run_date ë° ticker ì¶”ê°€
                for item in news_items:
                    item['run_date'] = run_date
                    item['ticker'] = ticker
                
                return {
                    'success': True,
                    'ticker': ticker,
                    'news_items': news_items,
                    'count': len(news_items)
                }
            else:
                return {
                    'success': True,
                    'ticker': ticker,
                    'news_items': [],
                    'count': 0
                }
        
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {ticker} - {e}")
            return {
                'success': False,
                'ticker': ticker,
                'error': str(e)
            }
    
    # ë³‘ë ¬ ì²˜ë¦¬ (ìµœëŒ€ 5ê°œ ìŠ¤ë ˆë“œ ë™ì‹œ ì‹¤í–‰)
    with ThreadPoolExecutor(max_workers=5) as executor:
        # ëª¨ë“  ì¢…ëª©ì— ëŒ€í•´ ë¹„ë™ê¸° ì‘ì—… ì œì¶œ
        future_to_candidate = {
            executor.submit(fetch_single_news, candidate): candidate 
            for candidate in candidates
        }
        
        # ì™„ë£Œëœ ì‘ì—…ë¶€í„° ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
        for future in as_completed(future_to_candidate):
            result = future.result()
            
            if result['success']:
                ticker = result['ticker']
                news_items = result.get('news_items', [])
                
                if news_items:
                    # DB ì €ì¥ (ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì•ˆì „í•˜ê²Œ)
                    db.insert_news_items(news_items)
                    total_news += result['count']
                    db.update_candidate_status(run_date, ticker, 'news_collected')
                    logger.debug(f"âœ“ {ticker}: {result['count']}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
                else:
                    logger.warning(f"âœ— {ticker}: ë‰´ìŠ¤ ì—†ìŒ")
                    db.update_candidate_status(run_date, ticker, 'no_news')
            else:
                ticker = result['ticker']
                db.update_candidate_status(run_date, ticker, 'news_failed')
                failed_tickers += 1
                errors.append(f"News collection failed for {ticker}: {result.get('error', 'Unknown')}")
            
            # Rate limiting (ì „ì²´ì ìœ¼ë¡œ)
            time.sleep(0.1)
    
    return {
        'total_news': total_news,
        'failed_tickers': failed_tickers,
        'errors': errors
    }


def generate_summaries(run_date: str, candidates: List[Dict], 
                      db: StudyDatabase) -> Dict:
    """
    LLMìœ¼ë¡œ ì¢…ëª© ìš”ì•½ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
    
    Returns:
        {'success_count': int, 'failed_count': int, 'errors': []}
    """
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        logger.warning("GEMINI_API_KEY not found. Skipping summaries.")
        return {'success_count': 0, 'failed_count': 0, 'errors': ['No API key']}
    
    success_count = 0
    failed_count = 0
    errors = []
    
    # ìš”ì•½ì´ í•„ìš”í•œ ì¢…ëª©ë§Œ í•„í„°ë§ (ìºì‹±)
    stocks_to_summarize = []
    for candidate in candidates:
        ticker = candidate['ticker']
        
        # ì´ë¯¸ ìš”ì•½ì´ ìˆëŠ”ì§€ í™•ì¸
        if db.has_summary(run_date, ticker):
            logger.debug(f"Summary already exists for {ticker}, skipping")
            continue
        
        stocks_to_summarize.append({
            'ticker': ticker,
            'name': candidate['name']
        })
    
    if not stocks_to_summarize:
        logger.info("No new summaries needed (all cached)")
        return {'success_count': 0, 'failed_count': 0, 'errors': []}
    
    # Gemini API ì„¤ì • - 2.5 Proë¡œ ì—…ê·¸ë ˆì´ë“œ (ë” ì •í™•í•œ ìš”ì•½)
    try:
        genai.configure(api_key=api_key)
        model_name = os.getenv('GEMINI_MODEL', 'gemini-2.5-pro')
        model = genai.GenerativeModel(model_name)
        logger.info(f"Using Gemini model: {model_name}")
        
        # ë°°ì¹˜ í¬ê¸° ì„¤ì • (Pro ëª¨ë¸ì€ ë” ëŠë¦¬ë¯€ë¡œ ì¤„ì„)
        batch_size = int(os.getenv('LLM_BATCH_SIZE', '5'))
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(stocks_to_summarize), batch_size):
            batch = stocks_to_summarize[i:i + batch_size]
            
            logger.info(f"ë°°ì¹˜ ìš”ì•½ ìƒì„± ì¤‘ ({i+1}-{i+len(batch)}/{len(stocks_to_summarize)})")
            
            try:
                summaries = get_batch_summaries_gemini(batch, model, run_date, db)
                
                for ticker, summary_data in summaries.items():
                    if summary_data['success']:
                        success_count += 1
                        db.update_candidate_status(run_date, ticker, 'summarized')
                    else:
                        failed_count += 1
                        errors.append(f"Summary failed for {ticker}")
                
                # Rate limiting
                time.sleep(2)
            
            except Exception as e:
                logger.error(f"Batch summary failed: {e}")
                failed_count += len(batch)
                errors.append(f"Batch summary error: {e}")
    
    except Exception as e:
        logger.error(f"Gemini API setup failed: {e}", exc_info=True)
        errors.append(f"Gemini setup failed: {e}")
    
    return {
        'success_count': success_count,
        'failed_count': failed_count,
        'errors': errors
    }


def generate_study_notes(run_date: str, candidates: List[Dict], 
                        db: StudyDatabase) -> Dict:
    """
    ë°±ì¼ê³µë¶€ í•™ìŠµ ë©”ëª¨ ìƒì„± (ì‚¬ì‹¤ ê²€ì¦ â†’ í•™ìŠµ í¬ì¸íŠ¸ ì¶”ì¶œ)
    
    Returns:
        {'success_count': int, 'failed_count': int, 'errors': []}
    """
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        logger.warning("GEMINI_API_KEY not found. Skipping study notes.")
        return {'success_count': 0, 'failed_count': 0, 'errors': ['No API key']}
    
    success_count = 0
    failed_count = 0
    errors = []
    
    # í•™ìŠµ ë©”ëª¨ê°€ í•„ìš”í•œ ì¢…ëª©ë§Œ í•„í„°ë§
    stocks_to_note = []
    for candidate in candidates:
        ticker = candidate['ticker']
        
        # ì´ë¯¸ í•™ìŠµ ë©”ëª¨ê°€ ìˆëŠ”ì§€ í™•ì¸
        if db.has_study_note(run_date, ticker):
            logger.debug(f"Study note already exists for {ticker}, skipping")
            continue
        
        # ë‰´ìŠ¤ê°€ ìˆëŠ” ì¢…ëª©ë§Œ ì²˜ë¦¬
        news_items = db.get_news_items(run_date, ticker)
        if not news_items:
            continue
        
        stocks_to_note.append({
            'ticker': ticker,
            'name': candidate['name'],
            'news_count': len(news_items)
        })
    
    if not stocks_to_note:
        logger.info("No new study notes needed")
        return {'success_count': 0, 'failed_count': 0, 'errors': []}
    
    # Gemini API ì„¤ì •
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash-exp')
        
        # ë°°ì¹˜ í¬ê¸° (í•™ìŠµ ë©”ëª¨ëŠ” ë” ì‹ ì¤‘í•˜ê²Œ 3ê°œì”©)
        batch_size = 3
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(stocks_to_note), batch_size):
            batch = stocks_to_note[i:i + batch_size]
            
            logger.info(f"ë°°ì¹˜ í•™ìŠµ ë©”ëª¨ ìƒì„± ì¤‘ ({i+1}-{i+len(batch)}/{len(stocks_to_note)})")
            
            try:
                notes = get_batch_study_notes_gemini(batch, model, run_date, db)
                
                for ticker, note_data in notes.items():
                    if note_data['success']:
                        success_count += 1
                        logger.info(f"âœ“ {ticker}: í•™ìŠµ ë©”ëª¨ ìƒì„± ì™„ë£Œ (ì‹ ë¢°ë„: {note_data.get('confidence', 'unknown')})")
                    else:
                        failed_count += 1
                        errors.append(f"Study note failed for {ticker}")
                
                # Rate limiting (í•™ìŠµ ë©”ëª¨ëŠ” ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
                time.sleep(3)
            
            except Exception as e:
                logger.error(f"Batch study note failed: {e}")
                failed_count += len(batch)
                errors.append(f"Batch study note error: {e}")
    
    except Exception as e:
        logger.error(f"Gemini API setup failed: {e}", exc_info=True)
        errors.append(f"Gemini setup failed: {e}")
    
    return {
        'success_count': success_count,
        'failed_count': failed_count,
        'errors': errors
    }


def get_batch_study_notes_gemini(stocks: List[Dict], model, run_date: str,
                                 db: StudyDatabase) -> Dict:
    """
    Gemini APIë¡œ ë°±ì¼ê³µë¶€ í•™ìŠµ ë©”ëª¨ ë°°ì¹˜ ìƒì„±
    
    ë°±ì¼ê³µë¶€ ì² í•™:
    1. ì‚¬ì‹¤ ìˆ˜ì§‘ â†’ 2. ì‚¬ì‹¤ ìš”ì•½ â†’ 3. ê²€ì¦ â†’ 4. í•™ìŠµ ë©”ëª¨ â†’ 5. ì‹ ë¢°ë„ í‰ê°€
    
    Returns:
        {ticker: {'success': bool, 'confidence': str}, ...}
    """
    results = {}
    
    try:
        # ê° ì¢…ëª©ì˜ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘
        stock_news_map = {}
        for stock in stocks:
            ticker = stock['ticker']
            news_items = db.get_news_items(run_date, ticker)
            
            # ë‰´ìŠ¤ ì œëª©ê³¼ ìš”ì•½ë§Œ ì¶”ì¶œ
            news_texts = []
            for news in news_items[:10]:  # ìµœëŒ€ 10ê°œë§Œ ì‚¬ìš©
                news_texts.append(f"- [{news.get('publisher', 'ì¶œì²˜ë¶ˆëª…')}] {news['title']}: {news.get('snippet', '')}")
            
            stock_news_map[ticker] = {
                'name': stock['name'],
                'news_texts': '\n'.join(news_texts) if news_texts else '(ë‰´ìŠ¤ ì—†ìŒ)'
            }
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ë°±ì¼ê³µë¶€ ì² í•™ ë°˜ì˜)
        stock_sections = []
        for ticker, info in stock_news_map.items():
            stock_sections.append(
                f"### {info['name']} ({ticker})\n"
                f"ê´€ë ¨ ë‰´ìŠ¤:\n{info['news_texts']}\n"
            )
        
        stocks_text = "\n".join(stock_sections)
        
        prompt = f"""ë‹¹ì‹ ì€ "ì£¼ì‹ ê³µë¶€ìš© í•™ìŠµ ë©”ëª¨"ë¥¼ ì‘ì„±í•˜ëŠ” AIì…ë‹ˆë‹¤. 
ì•„ë˜ ì¢…ëª©ë“¤ì— ëŒ€í•´ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ê³ , ê° ì¢…ëª©ë§ˆë‹¤ ë‹¤ìŒ í˜•ì‹ì˜ JSONì„ ìƒì„±í•˜ì„¸ìš”:

**ë°±ì¼ê³µë¶€ ì›ì¹™:**
1. ì‚¬ì‹¤ë§Œ ì¶”ì¶œ (ì¶”ì¸¡/ì˜ˆì¸¡ ê¸ˆì§€)
2. ì—¬ëŸ¬ ê¸°ì‚¬ì—ì„œ ê³µí†µìœ¼ë¡œ ë°˜ë³µë˜ëŠ” ë‚´ìš©ë§Œ ìš”ì•½
3. í•™ìŠµ ë©”ëª¨ëŠ” "ì´ ì¢…ëª©ì—ì„œ ë°°ìš¸ ì "ì„ ì¼ë°˜í™”ëœ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±
4. ì‹ ë¢°ë„ í‰ê°€: high(3ê°œ ì´ìƒ ê¸°ì‚¬ ì¼ì¹˜), mid(2ê°œ ê¸°ì‚¬ ì¼ì¹˜), low(ë‹¨ì¼ ê¸°ì‚¬ ë˜ëŠ” ë¶ˆëª…í™•)

**ì¶œë ¥ í˜•ì‹ (JSON):**
```json
{{
  "ì¢…ëª©ì½”ë“œ": {{
    "factual_summary": "ì—¬ëŸ¬ ê¸°ì‚¬ì—ì„œ ê³µí†µìœ¼ë¡œ ì–¸ê¸‰ëœ ì‚¬ì‹¤ë§Œ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½. ë‹¨ì¼ ê¸°ì‚¬ ì£¼ì¥ì€ ì œì™¸.",
    "ai_learning_note": "ì´ ì¢…ëª©ì—ì„œ ë°°ìš¸ ìˆ˜ ìˆëŠ” ì¼ë°˜í™”ëœ êµí›ˆ. íŠ¹ì • ì¢…ëª©ëª… ì–¸ê¸‰ ê¸ˆì§€. ë‹¤ìŒì— ë¹„ìŠ·í•œ íŒ¨í„´ì„ ë§Œë‚¬ì„ ë•Œ ì²´í¬í•  ì¡°ê±´ í¬í•¨. ê°ì •/ì˜ˆì¸¡/ê¶Œìœ  ê¸ˆì§€.",
    "ai_confidence": "high ë˜ëŠ” mid ë˜ëŠ” low",
    "verification_status": "ê¸°ì‚¬ ê°„ ë‚´ìš© ì¼ì¹˜ ì—¬ë¶€ ë˜ëŠ” 'í™•ì¸ í•„ìš”' ë©”ì‹œì§€"
  }}
}}
```

**ì˜ˆì‹œ:**
```json
{{
  "123456": {{
    "factual_summary": "ë³µìˆ˜ì˜ ì–¸ë¡ ì‚¬ê°€ Aì‚¬ì™€ì˜ ê³„ì•½ ì²´ê²°ì„ ë³´ë„. ê³„ì•½ ê·œëª¨ëŠ” 100ì–µì›ìœ¼ë¡œ ì¼ì¹˜.",
    "ai_learning_note": "ì£¼ìš” ê³ ê°ì‚¬ì™€ì˜ ëŒ€ê·œëª¨ ê³„ì•½ ì²´ê²° ì‹œ ë‹¨ê¸° ê¸‰ë“± ê°€ëŠ¥ì„±. ê³„ì•½ ê·œëª¨, ê³ ê°ì‚¬ ì‹ ë¢°ë„, ê¸°ì¡´ ë§¤ì¶œ ëŒ€ë¹„ ë¹„ì¤‘ í™•ì¸ í•„ìš”.",
    "ai_confidence": "high",
    "verification_status": "3ê°œ ì–¸ë¡ ì‚¬ ë³´ë„ ë‚´ìš© ì¼ì¹˜"
  }}
}}
```

**ë¶„ì„í•  ì¢…ëª©:**
{stocks_text}

**ì¤‘ìš”:** JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.
"""
        
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        
        # JSON íŒŒì‹±
        response_text = response_text.replace("```json", "").replace("```", "").strip()
        start_idx = response_text.find('{')
        end_idx = response_text.rfind('}')
        
        if start_idx != -1 and end_idx != -1:
            json_text = response_text[start_idx:end_idx+1]
            json_response = json.loads(json_text)
            
            # DBì— ì €ì¥
            for ticker, note_data in json_response.items():
                try:
                    db.insert_study_note({
                        'run_date': run_date,
                        'ticker': ticker,
                        'factual_summary': note_data.get('factual_summary'),
                        'ai_learning_note': note_data.get('ai_learning_note'),
                        'ai_confidence': note_data.get('ai_confidence', 'low'),
                        'verification_status': note_data.get('verification_status')
                    })
                    
                    results[ticker] = {
                        'success': True, 
                        'confidence': note_data.get('ai_confidence', 'unknown')
                    }
                
                except Exception as e:
                    logger.error(f"Failed to save study note for {ticker}: {e}")
                    results[ticker] = {'success': False}
        else:
            logger.warning("Gemini ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            for stock in stocks:
                results[stock['ticker']] = {'success': False}
    
    except Exception as e:
        logger.error(f"Batch study note generation failed: {e}", exc_info=True)
        for stock in stocks:
            results[stock['ticker']] = {'success': False}
    
    return results


def get_batch_summaries_gemini(stocks: List[Dict], model, run_date: str, 
                               db: StudyDatabase) -> Dict:
    """
    Gemini APIë¡œ ë°°ì¹˜ ìš”ì•½ ìƒì„± (ë‰´ìŠ¤ ê¸°ë°˜ - í™˜ê° ë°©ì§€)
    
    Returns:
        {ticker: {'success': bool, 'summary': str}, ...}
    """
    results = {}
    
    try:
        # ê° ì¢…ëª©ì˜ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ (í™˜ê° ë°©ì§€)
        stock_news_map = {}
        for stock in stocks:
            ticker = stock['ticker']
            news_items = db.get_news_items(run_date, ticker)
            
            # ë‰´ìŠ¤ ì œëª©ê³¼ ìš”ì•½ë§Œ ì¶”ì¶œ (ìµœëŒ€ 5ê°œ)
            news_texts = []
            for news in news_items[:5]:
                news_texts.append(f"- [{news.get('publisher', '')}] {news['title']}")
                if news.get('snippet'):
                    news_texts.append(f"  {news.get('snippet')}")
            
            stock_news_map[ticker] = {
                'name': stock['name'],
                'news_texts': '\n'.join(news_texts) if news_texts else '(ë‰´ìŠ¤ ì—†ìŒ)'
            }
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ë‰´ìŠ¤ ê¸°ë°˜)
        stock_sections = []
        for ticker, info in stock_news_map.items():
            stock_sections.append(
                f"### {info['name']} ({ticker})\n"
                f"ê´€ë ¨ ë‰´ìŠ¤:\n{info['news_texts']}\n"
            )
        
        stocks_text = "\n".join(stock_sections)
        
        prompt = f"""ì•„ë˜ ì¢…ëª©ë“¤ì„ **ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ** ìš”ì•½í•˜ì„¸ìš”.

**ì¤‘ìš”:**
- ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ "ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ"ì´ë¼ê³ ë§Œ ì ìœ¼ì„¸ìš”
- ì¶”ì¸¡í•˜ì§€ ë§ê³  ë‰´ìŠ¤ì— ëª…ì‹œëœ ì‚¬ì‹¤ë§Œ ìš”ì•½
- ê° ì¢…ëª©ë‹¹ 2-4ë¬¸ì¥

**ì¶œë ¥ í˜•ì‹ (JSON):**
```json
{{
  "ì¢…ëª©ì½”ë“œ": "ë‰´ìŠ¤ ê¸°ë°˜ ìš”ì•½ ë‚´ìš©"
}}
```

**ì¢…ëª© ë° ë‰´ìŠ¤:**
{stocks_text}

**JSONë§Œ ì¶œë ¥:**
"""
        
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        
        # JSON íŒŒì‹±
        response_text = response_text.replace("```json", "").replace("```", "").strip()
        start_idx = response_text.find('{')
        end_idx = response_text.rfind('}')
        
        if start_idx != -1 and end_idx != -1:
            json_text = response_text[start_idx:end_idx+1]
            json_response = json.loads(json_text)
            
            # DBì— ì €ì¥
            for ticker, summary_text in json_response.items():
                try:
                    db.insert_summary({
                        'run_date': run_date,
                        'ticker': ticker,
                        'summary_text': summary_text,
                        'llm_provider': 'gemini',
                        'llm_model': 'gemini-2.0-flash-exp'
                    })
                    
                    results[ticker] = {'success': True, 'summary': summary_text}
                
                except Exception as e:
                    logger.error(f"Failed to save summary for {ticker}: {e}")
                    results[ticker] = {'success': False}
        else:
            logger.warning("Gemini ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            for stock in stocks:
                results[stock['ticker']] = {'success': False}
    
    except Exception as e:
        logger.error(f"Batch summary generation failed: {e}", exc_info=True)
        for stock in stocks:
            results[stock['ticker']] = {'success': False}
    
    return results


def backup_to_gsheet(run_date: str, db: StudyDatabase, notifier):
    """Google Sheets ë°±ì—… (ì˜µì…˜)"""
    try:
        from .study_legacy import get_gsheet_client, get_worksheet_or_create
        from gspread_dataframe import set_with_dataframe
        import pandas as pd
        
        # ë°ì´í„° ì¡°íšŒ
        data = db.get_full_study_data(run_date)
        candidates = data['candidates']
        summaries = data['summaries']
        
        if not candidates:
            return
        
        # DataFrame êµ¬ì„±
        records = []
        for candidate in candidates:
            ticker = candidate['ticker']
            summary = summaries.get(ticker, {})
            
            records.append({
                'ë‚ ì§œ': run_date,
                'ì¢…ëª©ì½”ë“œ': ticker,
                'ì¢…ëª©ëª…': candidate['name'],
                'ì„ ì •ì‚¬ìœ ': candidate['reason_flag'],
                'ì¢…ê°€': f"{candidate['close_price']:,}",
                'ë“±ë½ë¥ ': f"{candidate['change_pct']:.2f}%",
                'ê±°ë˜ëŸ‰': f"{candidate['volume']:,}",
                'ê¸°ì—…ê°œìš”': summary.get('summary_text', 'ìš”ì•½ ì—†ìŒ')
            })
        
        # Google Sheets ì—…ë°ì´íŠ¸
        gsheet_client = get_gsheet_client()
        spreadsheet = gsheet_client.open("ì‹œì¥ ê´€ì‹¬ì£¼ ì¶”ì ")
        log_ws = get_worksheet_or_create(spreadsheet, "DailyLog")
        
        # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
        existing_df = pd.DataFrame(log_ws.get_all_records())
        new_df = pd.DataFrame(records).astype(str)
        
        if not existing_df.empty:
            combined_df = pd.concat([existing_df, new_df], ignore_index=True)
        else:
            combined_df = new_df
        
        set_with_dataframe(log_ws, combined_df, include_index=False, resize=True)
        logger.info(f"Backed up {len(records)} records to Google Sheets")
    
    except Exception as e:
        raise Exception(f"GSheet backup failed: {e}")


def auto_commit_to_github(run_date: str, stats: Dict):
    """
    GitHub ìë™ ì»¤ë°‹ ë° í‘¸ì‹œ
    
    Args:
        run_date: ì‹¤í–‰ ë‚ ì§œ (YYYYMMDD)
        stats: ì‹¤í–‰ í†µê³„
    """
    import subprocess
    from pathlib import Path
    
    try:
        # Git ì €ì¥ì†Œ ë£¨íŠ¸ ê²½ë¡œ
        repo_root = Path(__file__).parent.parent.parent
        
        # Git lock íŒŒì¼ ì²´í¬ (ë™ì‹œ Git ì‘ì—… ë°©ì§€)
        lock_file = repo_root / '.git' / 'index.lock'
        if lock_file.exists():
            logger.warning("Git lock íŒŒì¼ ê°ì§€ë¨. ë‹¤ë¥¸ Git ì‘ì—…ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì»¤ë°‹ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        # data/study.db íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        db_file = repo_root / 'data' / 'study.db'
        if not db_file.exists():
            logger.warning("study.db íŒŒì¼ì´ ì—†ì–´ ì»¤ë°‹ ê±´ë„ˆëœ€")
            return
        
        # Git add (Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)
        result = subprocess.run(
            ['git', 'add', 'data/study.db'],
            cwd=repo_root,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore',  # ë””ì½”ë”© ì˜¤ë¥˜ ë¬´ì‹œ
            timeout=10
        )
        
        if result.returncode != 0:
            logger.warning(f"Git add ì‹¤íŒ¨: {result.stderr}")
            return
        
        # ë³€ê²½ì‚¬í•­ì´ ìˆëŠ”ì§€ í™•ì¸ (Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)
        result = subprocess.run(
            ['git', 'diff', '--cached', '--quiet'],
            cwd=repo_root,
            capture_output=True,
            encoding='utf-8',
            errors='ignore',
            timeout=10
        )
        
        # returncodeê°€ 1ì´ë©´ ë³€ê²½ì‚¬í•­ ìˆìŒ, 0ì´ë©´ ë³€ê²½ì‚¬í•­ ì—†ìŒ
        if result.returncode == 0:
            logger.info("ë³€ê²½ì‚¬í•­ì´ ì—†ì–´ ì»¤ë°‹ ê±´ë„ˆëœ€")
            return
        
        # Git commit (Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)
        commit_message = (
            f"ğŸ“š ìœ ëª©ë¯¼ ê³µë¶€ë²• ìë™ ì—…ë°ì´íŠ¸ ({run_date})\n\n"
            f"- í›„ë³´ ì¢…ëª©: {stats['candidates']}ê°œ\n"
            f"- ë‰´ìŠ¤ ìˆ˜ì§‘: {stats['news_collected']}ê°œ\n"
            f"- AI ìš”ì•½: {stats['summaries_generated']}ê°œ"
        )
        
        result = subprocess.run(
            ['git', 'commit', '-m', commit_message],
            cwd=repo_root,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore',
            timeout=10
        )
        
        if result.returncode != 0:
            logger.warning(f"Git commit ì‹¤íŒ¨: {result.stderr}")
            return
        
        logger.info(f"âœ“ Git commit ì™„ë£Œ: {commit_message.split()[0]}")
        
        # Git push (ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ - ë„¤íŠ¸ì›Œí¬ ì´ìŠˆ ë“±, Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)
        result = subprocess.run(
            ['git', 'push'],
            cwd=repo_root,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore',
            timeout=30
        )
        
        if result.returncode == 0:
            logger.info("âœ“ Git push ì™„ë£Œ â†’ GitHub ì—…ë°ì´íŠ¸ë¨")
        else:
            logger.warning(f"Git push ì‹¤íŒ¨ (ë¬´ì‹œë¨): {result.stderr}")
    
    except subprocess.TimeoutExpired:
        logger.warning("Git ëª…ë ¹ íƒ€ì„ì•„ì›ƒ")
    except Exception as e:
        logger.warning(f"Git ìë™ ì»¤ë°‹ ì¤‘ ì˜¤ë¥˜: {e}")


def send_completion_notification(run_date: str, stats: Dict, notifier, db: StudyDatabase):
    """ì™„ë£Œ ì•Œë¦¼ ë°œì†¡"""
    try:
        # ìƒìœ„ 5ê°œ ì¢…ëª© ì •ë³´
        candidates = db.get_candidates(run_date)[:5]
        
        fields = []
        for candidate in candidates:
            fields.append({
                "name": f"ğŸ“Š {candidate['name']} ({candidate['ticker']})",
                "value": f"ë“±ë½ë¥ : {candidate['change_pct']:.2f}% | ì‚¬ìœ : {candidate['reason_flag']}",
                "inline": False
            })
        
        embed = {
            "title": f"ğŸ“š ìœ ëª©ë¯¼ ê³µë¶€ë²• ì™„ë£Œ ({run_date})",
            "description": (
                f"âœ… í›„ë³´ ì¢…ëª©: **{stats['candidates']}ê°œ**\n"
                f"ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘: **{stats['news_collected']}ê°œ**\n"
                f"ğŸ¤– AI ìš”ì•½: **{stats['summaries_generated']}ê°œ**\n"
                f"âš ï¸ ì˜¤ë¥˜: **{len(stats['errors'])}ê±´**"
            ),
            "color": 5814783,
            "fields": fields,
            "footer": {"text": f"SQLite DB ì €ì¥ ì™„ë£Œ | ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸ ê°€ëŠ¥"}
        }
        
        notifier.send_alert("ìœ ëª©ë¯¼ ê³µë¶€ë²• ì™„ë£Œ", embed=embed)
    
    except Exception as e:
        logger.error(f"Failed to send notification: {e}")


# ==================== CLI ì¸í„°í˜ì´ìŠ¤ ====================

if __name__ == '__main__':
    import argparse
    from pathlib import Path
    from dotenv import load_dotenv
    from .notifier import Notifier
    
    # .env íŒŒì¼ ëª…ì‹œì  ë¡œë“œ
    env_path = Path(__file__).parent.parent.parent / 'configs' / '.env'
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… .env íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {env_path}")
    else:
        print(f"âš ï¸ .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {env_path}")
    
    parser = argparse.ArgumentParser(description='ìœ ëª©ë¯¼ ê³µë¶€ë²• ìˆ˜ë™ ì‹¤í–‰')
    parser.add_argument('--force', action='store_true', help='ê°•ì œ ì‹¤í–‰ (ì¤‘ë³µ ë¬´ì‹œ)')
    parser.add_argument('--date', type=str, help='íŠ¹ì • ë‚ ì§œ ì‹¤í–‰ (YYYYMMDD)')
    parser.add_argument('--news-only', action='store_true', help='ë‰´ìŠ¤ë§Œ ì¬ìˆ˜ì§‘')
    
    args = parser.parse_args()
    
    # Notifier ì´ˆê¸°í™”
    notifier = Notifier()
    
    if args.date:
        # íŠ¹ì • ë‚ ì§œë¡œ ì‹¤í–‰
        run_daily_study(None, notifier, force_run=args.force, target_date=args.date)
    else:
        # ìµœê·¼ ê±°ë˜ì¼ë¡œ ì‹¤í–‰
        run_daily_study(None, notifier, force_run=args.force)
