"""
ë°ì´í„° ë°±ì—… ë° ì•Œë¦¼ ë°œì†¡ ëª¨ë“ˆ
"""
import os
import shutil
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional

from hantubot.reporting.logger import get_logger
from hantubot.study.repository import StudyDatabase
from hantubot.utils.config_loader import load_config_with_env

logger = get_logger(__name__)


def backup_database():
    """
    study.db ìë™ ë°±ì—… (config.yaml ì„¤ì • ê¸°ë°˜)
    
    - ì¼ìš”ì¼ë§ˆë‹¤ ìë™ ë°±ì—…
    - ì„¤ì •ëœ ê¸°ê°„(ê¸°ë³¸ 30ì¼) ì´ìƒ ëœ ë°±ì—… ìë™ ì‚­ì œ
    """
    try:
        config = load_config_with_env('configs/config.yaml')
        study_config = config.get('study', {})
        
        # ë°±ì—… í™œì„±í™” ì²´í¬
        if not study_config.get('enable_auto_backup', True):
            logger.debug("DB ìë™ ë°±ì—…ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        db_path = Path('data/study.db')
        if not db_path.exists():
            logger.warning("study.db íŒŒì¼ì´ ì—†ì–´ ë°±ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        backup_dir = Path('data/backups')
        backup_dir.mkdir(exist_ok=True)
        
        # ì¼ìš”ì¼(weekday=6)ë§ˆë‹¤ ë°±ì—…
        now = datetime.now()
        if now.weekday() == 6:
            backup_file = backup_dir / f"study_backup_{now:%Y%m%d}.db"
            
            # ì´ë¯¸ ì˜¤ëŠ˜ ë°±ì—…ì´ ìˆìœ¼ë©´ ê±´ë„ˆëœ€
            if backup_file.exists():
                logger.info(f"ì˜¤ëŠ˜ ë°±ì—…ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {backup_file}")
                return
            
            shutil.copy(db_path, backup_file)
            logger.info(f"âœ… DB ë°±ì—… ì™„ë£Œ: {backup_file}")
            
            # ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ
            retention_days = study_config.get('backup_retention_days', 30)
            deleted_count = 0
            for old_backup in backup_dir.glob("study_backup_*.db"):
                age_days = (now - datetime.fromtimestamp(old_backup.stat().st_mtime)).days
                if age_days > retention_days:
                    old_backup.unlink()
                    deleted_count += 1
                    logger.info(f"ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {old_backup} ({age_days}ì¼)")
            
            if deleted_count > 0:
                logger.info(f"ì´ {deleted_count}ê°œì˜ ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ ì™„ë£Œ")
        else:
            logger.debug(f"ë°±ì—… ì˜ˆì •ì¼ì´ ì•„ë‹™ë‹ˆë‹¤. (í˜„ì¬: {now.strftime('%A')}, ë°±ì—…ì¼: ì¼ìš”ì¼)")
    
    except Exception as e:
        logger.error(f"DB ë°±ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)


def backup_to_gsheet(run_date: str, db: StudyDatabase, notifier):
    """Google Sheets ë°±ì—… (ì˜µì…˜)"""
    try:
        from hantubot.reporting.study_legacy import get_gsheet_client, get_worksheet_or_create
        from gspread_dataframe import set_with_dataframe
        import pandas as pd
        
        # ë°ì´í„° ì¡°íšŒ
        data = db.get_full_study_data(run_date)
        candidates = data['candidates']
        summaries = data['summaries']
        
        if not candidates:
            return
        
        # DataFrame êµ¬ì„±
        records = []
        for candidate in candidates:
            ticker = candidate['ticker']
            summary = summaries.get(ticker, {})
            
            records.append({
                'ë‚ ì§œ': run_date,
                'ì¢…ëª©ì½”ë“œ': ticker,
                'ì¢…ëª©ëª…': candidate['name'],
                'ì„ ì •ì‚¬ìœ ': candidate['reason_flag'],
                'ì¢…ê°€': f"{candidate['close_price']:,}",
                'ë“±ë½ë¥ ': f"{candidate['change_pct']:.2f}%",
                'ê±°ë˜ëŸ‰': f"{candidate['volume']:,}",
                'ê¸°ì—…ê°œìš”': summary.get('summary_text', 'ìš”ì•½ ì—†ìŒ')
            })
        
        # Google Sheets ì—…ë°ì´íŠ¸
        gsheet_client = get_gsheet_client()
        spreadsheet = gsheet_client.open("ì‹œì¥ ê´€ì‹¬ì£¼ ì¶”ì ")
        log_ws = get_worksheet_or_create(spreadsheet, "DailyLog")
        
        # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
        existing_df = pd.DataFrame(log_ws.get_all_records())
        new_df = pd.DataFrame(records).astype(str)
        
        if not existing_df.empty:
            combined_df = pd.concat([existing_df, new_df], ignore_index=True)
        else:
            combined_df = new_df
        
        set_with_dataframe(log_ws, combined_df, include_index=False, resize=True)
        logger.info(f"Backed up {len(records)} records to Google Sheets")
    
    except Exception as e:
        raise Exception(f"GSheet backup failed: {e}")


def auto_commit_to_github(run_date: str, stats: Dict):
    """
    GitHub ìë™ ì»¤ë°‹ ë° í‘¸ì‹œ
    
    Args:
        run_date: ì‹¤í–‰ ë‚ ì§œ (YYYYMMDD)
        stats: ì‹¤í–‰ í†µê³„
    """
    try:
        # Git ì €ì¥ì†Œ ë£¨íŠ¸ ê²½ë¡œ
        repo_root = Path(__file__).parent.parent.parent
        
        # Git lock íŒŒì¼ ì²´í¬ (ë™ì‹œ Git ì‘ì—… ë°©ì§€)
        lock_file = repo_root / '.git' / 'index.lock'
        if lock_file.exists():
            logger.warning("Git lock íŒŒì¼ ê°ì§€ë¨. ë‹¤ë¥¸ Git ì‘ì—…ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì»¤ë°‹ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        # data/study.db íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        db_file = repo_root / 'data' / 'study.db'
        if not db_file.exists():
            logger.warning("study.db íŒŒì¼ì´ ì—†ì–´ ì»¤ë°‹ ê±´ë„ˆëœ€")
            return
        
        # Git add (Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)
        result = subprocess.run(
            ['git', 'add', 'data/study.db'],
            cwd=repo_root,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore',  # ë””ì½”ë”© ì˜¤ë¥˜ ë¬´ì‹œ
            timeout=10
        )
        
        if result.returncode != 0:
            logger.warning(f"Git add ì‹¤íŒ¨: {result.stderr}")
            return
        
        # ë³€ê²½ì‚¬í•­ì´ ìˆëŠ”ì§€ í™•ì¸ (Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)
        result = subprocess.run(
            ['git', 'diff', '--cached', '--quiet'],
            cwd=repo_root,
            capture_output=True,
            encoding='utf-8',
            errors='ignore',
            timeout=10
        )
        
        # returncodeê°€ 1ì´ë©´ ë³€ê²½ì‚¬í•­ ìˆìŒ, 0ì´ë©´ ë³€ê²½ì‚¬í•­ ì—†ìŒ
        if result.returncode == 0:
            logger.info("ë³€ê²½ì‚¬í•­ì´ ì—†ì–´ ì»¤ë°‹ ê±´ë„ˆëœ€")
            return
        
        # Git commit (Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)
        commit_message = (
            f"ğŸ“š ìœ ëª©ë¯¼ ê³µë¶€ë²• ìë™ ì—…ë°ì´íŠ¸ ({run_date})\n\n"
            f"- í›„ë³´ ì¢…ëª©: {stats['candidates']}ê°œ\n"
            f"- ë‰´ìŠ¤ ìˆ˜ì§‘: {stats['news_collected']}ê°œ\n"
            f"- AI ìš”ì•½: {stats['summaries_generated']}ê°œ"
        )
        
        result = subprocess.run(
            ['git', 'commit', '-m', commit_message],
            cwd=repo_root,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore',
            timeout=10
        )
        
        if result.returncode != 0:
            logger.warning(f"Git commit ì‹¤íŒ¨: {result.stderr}")
            return
        
        logger.info(f"âœ“ Git commit ì™„ë£Œ: {commit_message.split()[0]}")
        
        # Git push (ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ - ë„¤íŠ¸ì›Œí¬ ì´ìŠˆ ë“±, Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)
        result = subprocess.run(
            ['git', 'push'],
            cwd=repo_root,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore',
            timeout=30
        )
        
        if result.returncode == 0:
            logger.info("âœ“ Git push ì™„ë£Œ â†’ GitHub ì—…ë°ì´íŠ¸ë¨")
        else:
            logger.warning(f"Git push ì‹¤íŒ¨ (ë¬´ì‹œë¨): {result.stderr}")
    
    except subprocess.TimeoutExpired:
        logger.warning("Git ëª…ë ¹ íƒ€ì„ì•„ì›ƒ")
    except Exception as e:
        logger.warning(f"Git ìë™ ì»¤ë°‹ ì¤‘ ì˜¤ë¥˜: {e}")


def send_completion_notification(run_date: str, stats: Dict, notifier, db: StudyDatabase):
    """ì™„ë£Œ ì•Œë¦¼ ë°œì†¡"""
    try:
        # ìƒìœ„ 5ê°œ ì¢…ëª© ì •ë³´
        candidates = db.get_candidates(run_date)[:5]
        
        fields = []
        for candidate in candidates:
            fields.append({
                "name": f"ğŸ“Š {candidate['name']} ({candidate['ticker']})",
                "value": f"ë“±ë½ë¥ : {candidate['change_pct']:.2f}% | ì‚¬ìœ : {candidate['reason_flag']}",
                "inline": False
            })
        
        embed = {
            "title": f"ğŸ“š ìœ ëª©ë¯¼ ê³µë¶€ë²• ì™„ë£Œ ({run_date})",
            "description": (
                f"âœ… í›„ë³´ ì¢…ëª©: **{stats['candidates']}ê°œ**\n"
                f"ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘: **{stats['news_collected']}ê°œ**\n"
                f"ğŸ¤– AI ìš”ì•½: **{stats['summaries_generated']}ê°œ**\n"
                f"âš ï¸ ì˜¤ë¥˜: **{len(stats['errors'])}ê±´**"
            ),
            "color": 5814783,
            "fields": fields,
            "footer": {"text": f"SQLite DB ì €ì¥ ì™„ë£Œ | ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸ ê°€ëŠ¥"}
        }
        
        notifier.send_alert("ìœ ëª©ë¯¼ ê³µë¶€ë²• ì™„ë£Œ", embed=embed)
    
    except Exception as e:
        logger.error(f"Failed to send notification: {e}")
